{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Download ZIP file\n",
        "zip_path = \"/content/sample_data/user-wallet-transactions.json.zip\"\n",
        "url = \"https://drive.google.com/uc?id=14ceBCLQ-BTcydDrFJauVA_PKAZ7VtDor\"\n",
        "gdown.download(url, zip_path, quiet=False)\n",
        "\n",
        "# Extract only the main JSON file\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    # Get list of files and select the primary JSON file\n",
        "    file_list = z.namelist()\n",
        "    json_file = [f for f in file_list\n",
        "                 if \"user-wallet-transactions.json\" in f and \"__MACOSX\" not in f][0]\n",
        "\n",
        "    # Extract the target JSON file\n",
        "    z.extract(json_file)\n",
        "\n",
        "# Load data from extracted JSON\n",
        "df = pd.read_json(json_file, convert_dates=['timestamp'])\n",
        "\n",
        "print(f\"Successfully loaded {len(df):,} transactions\")\n",
        "print(\"Columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_30Ls9pZxwqt",
        "outputId": "256a1586-658d-46c3-a4a6-7fb53b54812f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14ceBCLQ-BTcydDrFJauVA_PKAZ7VtDor\n",
            "To: /content/sample_data/user-wallet-transactions.json.zip\n",
            "100%|██████████| 10.4M/10.4M [00:00<00:00, 61.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 100,000 transactions\n",
            "Columns: ['_id', 'userWallet', 'network', 'protocol', 'txHash', 'logId', 'timestamp', 'blockNumber', 'action', 'actionData', '__v', 'createdAt', 'updatedAt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio pandas numpy matplotlib seaborn scikit-learn yellowbrick fpdf yfinance networkx shap diffprivlib joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_H7Ac9ezXWA",
        "outputId": "b3c9df38-3b1e-4f38-e120-af5f278d555e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.11/dist-packages (1.5)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: diffprivlib in /usr/local/lib/python3.11/dist-packages (0.6.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.7.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=49.0.0 in /usr/local/lib/python3.11/dist-packages (from diffprivlib) (75.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, classification_report, silhouette_score  # Added silhouette_score here\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.manifold import TSNE\n",
        "import json\n",
        "import os\n",
        "from fpdf import FPDF\n",
        "import yfinance as yf\n",
        "import networkx as nx\n",
        "import shap\n",
        "import joblib\n",
        "from diffprivlib.models import KMeans as DPKMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.stats import zscore\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Rest of your code remains the same...\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "MAX_CLUSTERS = 10  # Max clusters for elbow method\n",
        "N_CLUSTERS = None  # Will be determined dynamically\n",
        "MIN_SAMPLES = 5     # Minimum samples per cluster\n",
        "REPORT_PATH = \"reports/\"\n",
        "os.makedirs(REPORT_PATH, exist_ok=True)\n",
        "\n",
        "def process_transactions(df):\n",
        "    \"\"\"Process transaction data and extract wallet-level features using vectorized operations\"\"\"\n",
        "    print(\"Processing transactions...\")\n",
        "\n",
        "    # Use 'userWallet' as wallet identifier\n",
        "    wallet_col = 'userWallet'\n",
        "\n",
        "    # Convert timestamp to datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    # Parse actionData - handle both string and dict formats\n",
        "    if isinstance(df['actionData'].iloc[0], str):\n",
        "        try:\n",
        "            df['actionData'] = df['actionData'].apply(json.loads)\n",
        "        except:\n",
        "            df['actionData'] = df['actionData'].apply(lambda x: {} if pd.isna(x) else json.loads(x))\n",
        "\n",
        "    # Extract relevant fields from actionData\n",
        "    df['actionType'] = df['actionData'].apply(lambda x: x.get('type', '').lower() if isinstance(x, dict) else '')\n",
        "    df['amount'] = df['actionData'].apply(lambda x: float(x.get('amount', 0)) if isinstance(x, dict) else 0)\n",
        "    df['assetPriceUSD'] = df['actionData'].apply(lambda x: float(x.get('assetPriceUSD', 0)) if isinstance(x, dict) else 0)\n",
        "    df['amountUSD'] = df['amount'] * df['assetPriceUSD']\n",
        "\n",
        "    # Add protocol information if available\n",
        "    df['protocol'] = df['actionData'].apply(\n",
        "        lambda x: x.get('protocol', 'unknown') if isinstance(x, dict) else 'unknown'\n",
        "    )\n",
        "\n",
        "    # Feature engineering using vectorized operations\n",
        "    print(\"Performing vectorized feature engineering...\")\n",
        "\n",
        "    # Group by wallet\n",
        "    grouped = df.groupby(wallet_col)\n",
        "\n",
        "    # Basic transaction counts\n",
        "    features = grouped.agg(\n",
        "        total_tx=('actionType', 'size'),\n",
        "        deposit_count=('actionType', lambda x: (x == 'deposit').sum()),\n",
        "        borrow_count=('actionType', lambda x: (x == 'borrow').sum()),\n",
        "        repay_count=('actionType', lambda x: (x == 'repay').sum()),\n",
        "        redeem_count=('actionType', lambda x: (x == 'redeemunderlying').sum()),\n",
        "        liquidation_count=('actionType', lambda x: (x == 'liquidationcall').sum()),\n",
        "        first_tx=('timestamp', 'min'),\n",
        "        last_tx=('timestamp', 'max'),\n",
        "        unique_protocols=('protocol', pd.Series.nunique)\n",
        "    )\n",
        "\n",
        "    # Time-based features\n",
        "    features['activity_duration_days'] = (\n",
        "        features['last_tx'] - features['first_tx']\n",
        "    ).dt.total_seconds() / 86400\n",
        "    features['tx_frequency'] = features['total_tx'] / features['activity_duration_days'].replace(0, 1)\n",
        "    features['recency_days'] = (datetime.now() - features['last_tx']).dt.days\n",
        "\n",
        "    # FIXED: Create financial features using pivot_table instead of looping\n",
        "    financial_features = df.pivot_table(\n",
        "        index=wallet_col,\n",
        "        columns='actionType',\n",
        "        values='amountUSD',\n",
        "        aggfunc=['sum', 'mean', 'max', 'min'],\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    # Flatten multi-index columns\n",
        "    financial_features.columns = [f'{agg}_{action}' for agg, action in financial_features.columns]\n",
        "\n",
        "    # Select only the actions we need\n",
        "    actions = ['deposit', 'borrow', 'repay', 'redeemunderlying']\n",
        "    action_cols = [col for col in financial_features.columns if any(action in col for action in actions)]\n",
        "    financial_features = financial_features[action_cols]\n",
        "\n",
        "    # Merge with main features\n",
        "    features = features.join(financial_features)\n",
        "\n",
        "    # Ratios and risk metrics\n",
        "    features['deposit_borrow_ratio'] = (features['sum_deposit'] + 1) / (features['sum_borrow'] + 1)\n",
        "    features['repay_borrow_ratio'] = (features['sum_repay'] + 1) / (features['sum_borrow'] + 1)\n",
        "    features['redeem_deposit_ratio'] = (features['sum_redeemunderlying'] + 1) / (features['sum_deposit'] + 1)\n",
        "    features['net_utilization'] = (features['sum_deposit'] - features['sum_redeemunderlying']) / (features['sum_deposit'] + 1)\n",
        "\n",
        "    # Risk flags\n",
        "    features['has_liquidation'] = (features['liquidation_count'] > 0).astype(int)\n",
        "    features['high_frequency'] = (features['tx_frequency'] > 100).astype(int)\n",
        "\n",
        "    # Transaction trend analysis\n",
        "    def calculate_trend(group):\n",
        "        if len(group) > 1:\n",
        "            x = np.arange(len(group))\n",
        "            y = group['amountUSD'].values\n",
        "            return np.polyfit(x, y, 1)[0]  # Return slope\n",
        "        return 0\n",
        "\n",
        "    trends = df.groupby(wallet_col).apply(calculate_trend)\n",
        "    features['tx_trend'] = trends\n",
        "\n",
        "    # Fill NA values\n",
        "    features.fillna(0, inplace=True)\n",
        "\n",
        "    # Replace infinities\n",
        "    features.replace([np.inf, -np.inf], 1e6, inplace=True)\n",
        "\n",
        "    # Create interaction features\n",
        "    features['borrow_recency_ratio'] = features['sum_borrow'] / (features['recency_days'] + 1)\n",
        "    features['deposit_frequency_ratio'] = features['sum_deposit'] / (features['tx_frequency'] + 1)\n",
        "\n",
        "    # Protocol diversity\n",
        "    features['protocol_diversity'] = features['unique_protocols'] / features['total_tx']\n",
        "\n",
        "    print(f\"Processed features for {len(features)} wallets\")\n",
        "    return features\n",
        "\n",
        "def optimize_clusters(X_scaled):\n",
        "    \"\"\"Determine optimal number of clusters using elbow method and silhouette score\"\"\"\n",
        "    print(\"Optimizing cluster count...\")\n",
        "\n",
        "    # Visual elbow method\n",
        "    visualizer = KElbowVisualizer(\n",
        "        KMeans(random_state=RANDOM_STATE),\n",
        "        k=(2, MAX_CLUSTERS),\n",
        "        metric='distortion',\n",
        "        timings=False\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    visualizer.fit(X_scaled)\n",
        "    visualizer.show(outpath=\"elbow_plot.png\", clear_figure=True)\n",
        "    plt.close()\n",
        "\n",
        "    # Silhouette score analysis\n",
        "    silhouette_scores = []\n",
        "    cluster_range = range(2, min(MAX_CLUSTERS, X_scaled.shape[0]//MIN_SAMPLES))\n",
        "\n",
        "    for n_clusters in cluster_range:\n",
        "        clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
        "        cluster_labels = clusterer.fit_predict(X_scaled)\n",
        "        score = silhouette_score(X_scaled, cluster_labels)\n",
        "        silhouette_scores.append(score)\n",
        "        print(f\"Silhouette score for {n_clusters} clusters: {score:.4f}\")\n",
        "\n",
        "    # Find best cluster count\n",
        "    best_n = cluster_range[np.argmax(silhouette_scores)]\n",
        "    print(f\"Optimal clusters: {best_n}\")\n",
        "\n",
        "    return best_n\n",
        "\n",
        "def generate_credit_scores(features_df):\n",
        "    \"\"\"Generate credit scores using supervised learning and clustering\"\"\"\n",
        "    print(\"Generating credit scores...\")\n",
        "\n",
        "    # Feature selection\n",
        "    feature_cols = [\n",
        "        'total_tx', 'deposit_count', 'borrow_count', 'repay_count',\n",
        "        'tx_frequency', 'deposit_borrow_ratio', 'repay_borrow_ratio',\n",
        "        'redeem_deposit_ratio', 'net_utilization', 'has_liquidation',\n",
        "        'high_frequency', 'recency_days', 'tx_trend', 'protocol_diversity',\n",
        "        'borrow_recency_ratio', 'deposit_frequency_ratio'\n",
        "    ]\n",
        "\n",
        "    # Preprocessing\n",
        "    X = features_df[feature_cols].fillna(0)\n",
        "\n",
        "    # Handle infinities\n",
        "    for col in ['deposit_borrow_ratio', 'repay_borrow_ratio', 'redeem_deposit_ratio']:\n",
        "        X[col] = X[col].replace([np.inf, -np.inf], X[col].max())\n",
        "\n",
        "    # Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Determine optimal clusters\n",
        "    global N_CLUSTERS\n",
        "    if N_CLUSTERS is None:\n",
        "        N_CLUSTERS = optimize_clusters(X_scaled)\n",
        "\n",
        "    # Differential Privacy Clustering\n",
        "    print(f\"Clustering with {N_CLUSTERS} clusters using differential privacy...\")\n",
        "    dp_kmeans = DPKMeans(\n",
        "        n_clusters=N_CLUSTERS,\n",
        "        random_state=RANDOM_STATE,\n",
        "        epsilon=0.5\n",
        "    )\n",
        "    cluster_labels = dp_kmeans.fit_predict(X_scaled)\n",
        "    features_df['cluster'] = cluster_labels\n",
        "\n",
        "    # Supervised Learning with Gradient Boosting\n",
        "    print(\"Training supervised credit model...\")\n",
        "    y = features_df['has_liquidation']\n",
        "    features_df = features_df.sort_values('last_tx')\n",
        "    split_idx = int(0.8 * len(features_df))\n",
        "    X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    gbm = GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    gbm.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = gbm.predict(X_test)\n",
        "    y_proba = gbm.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    print(f\"Model AUC: {auc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(gbm, 'credit_model.pkl')\n",
        "    joblib.dump(scaler, 'credit_scaler.pkl')\n",
        "\n",
        "    # Predict on entire dataset\n",
        "    full_proba = gbm.predict_proba(X_scaled)[:, 1]\n",
        "    features_df['default_probability'] = full_proba\n",
        "    features_df['credit_score'] = (1 - full_proba) * 1000\n",
        "\n",
        "    # Cluster quality metrics\n",
        "    cluster_stats = []\n",
        "    for i in range(N_CLUSTERS):\n",
        "        cluster_data = features_df[features_df['cluster'] == i]\n",
        "        stats = {\n",
        "            'cluster': i,\n",
        "            'size': len(cluster_data),\n",
        "            'risk_score': (\n",
        "                cluster_data['default_probability'].mean() * 0.6 +\n",
        "                cluster_data['high_frequency'].mean() * 0.2 +\n",
        "                (1 - cluster_data['repay_borrow_ratio'].mean()) * 0.2\n",
        "            ),\n",
        "            'avg_score': cluster_data['credit_score'].mean(),\n",
        "            'avg_tx_freq': cluster_data['tx_frequency'].mean(),\n",
        "            'avg_deposit_borrow_ratio': cluster_data['deposit_borrow_ratio'].mean(),\n",
        "            'avg_repay_borrow_ratio': cluster_data['repay_borrow_ratio'].mean(),\n",
        "            'liquidation_rate': cluster_data['has_liquidation'].mean(),\n",
        "            'high_freq_rate': cluster_data['high_frequency'].mean()\n",
        "        }\n",
        "        cluster_stats.append(stats)\n",
        "\n",
        "    cluster_stats_df = pd.DataFrame(cluster_stats)\n",
        "    cluster_stats_df = cluster_stats_df.sort_values('risk_score').reset_index(drop=True)\n",
        "    cluster_stats_df['quality_rank'] = cluster_stats_df.index\n",
        "    features_df['quality_rank'] = features_df['cluster'].map(cluster_stats_df.set_index('cluster')['quality_rank'])\n",
        "    # In generate_credit_scores() function:\n",
        "\n",
        "    print(\"Calculating SHAP values...\")\n",
        "    explainer = shap.TreeExplainer(gbm)\n",
        "    shap_values = explainer(X_scaled)\n",
        "\n",
        "    # Create Explanation object\n",
        "    shap_exp = shap.Explanation(\n",
        "        values=shap_values.values,\n",
        "        base_values=shap_values.base_values,\n",
        "        data=X_scaled,\n",
        "        feature_names=feature_cols\n",
        "    )\n",
        "\n",
        "    # Store the Explanation object for each wallet\n",
        "    features_df['shap_explanation'] = [shap_exp[i] for i in range(len(shap_exp))]\n",
        "\n",
        "\n",
        "\n",
        "    # Create SHAP plots\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_exp, X_scaled, feature_names=feature_cols, show=False)\n",
        "    plt.savefig(f'{REPORT_PATH}shap_summary.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Create results dataframe\n",
        "    results = pd.DataFrame({\n",
        "        'wallet': features_df.index,\n",
        "        'credit_score': features_df['credit_score'].astype(int),\n",
        "        'default_probability': features_df['default_probability'],\n",
        "        'cluster': features_df['cluster'],\n",
        "        'quality_rank': features_df['quality_rank']\n",
        "    }).sort_values('credit_score', ascending=False)\n",
        "\n",
        "    return results, cluster_stats_df, X_scaled, features_df\n",
        "\n",
        "\n",
        "def generate_tsne_visualization(X_scaled, scores_df):\n",
        "    \"\"\"Generate t-SNE visualization of wallet behavior clusters\"\"\"\n",
        "    print(\"Generating t-SNE visualization...\")\n",
        "\n",
        "    # Reduce dimensionality with PCA first for efficiency\n",
        "    n_components = min(10, X_scaled.shape[1])  # Use min of 10 or number of features\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # t-SNE visualization\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=30,\n",
        "        learning_rate=200,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_iter=1000\n",
        "    )\n",
        "\n",
        "    # Sample if too many points\n",
        "    if X_pca.shape[0] > 5000:\n",
        "        sample_idx = np.random.choice(X_pca.shape[0], 5000, replace=False)\n",
        "        X_sample = X_pca[sample_idx]\n",
        "        score_sample = scores_df.iloc[sample_idx]\n",
        "        print(\"Sampled 5000 wallets for t-SNE visualization\")\n",
        "    else:\n",
        "        X_sample = X_pca\n",
        "        score_sample = scores_df\n",
        "\n",
        "    # Fit and transform\n",
        "    tsne_results = tsne.fit_transform(X_sample)\n",
        "\n",
        "    # Create DataFrame for plotting\n",
        "    tsne_df = pd.DataFrame({\n",
        "        'x': tsne_results[:, 0],\n",
        "        'y': tsne_results[:, 1],\n",
        "        'credit_score': score_sample['credit_score'].values,\n",
        "        'cluster': score_sample['cluster'].values,\n",
        "        'score_category': pd.cut(\n",
        "            score_sample['credit_score'],\n",
        "            bins=[0, 300, 700, 1000],\n",
        "            labels=['High Risk (0-300)', 'Medium Risk (301-700)', 'Low Risk (701-1000)']\n",
        "        )\n",
        "    })\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Cluster visualization\n",
        "    plt.subplot(2, 1, 1)\n",
        "    sns.scatterplot(\n",
        "        x='x', y='y',\n",
        "        hue='cluster',\n",
        "        palette='viridis',\n",
        "        data=tsne_df,\n",
        "        alpha=0.7,\n",
        "        s=50\n",
        "    )\n",
        "    plt.title('Behavior Cluster Visualization (t-SNE Projection)')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend(title='Cluster ID')\n",
        "\n",
        "    # Risk category visualization\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.scatterplot(\n",
        "        x='x', y='y',\n",
        "        hue='score_category',\n",
        "        palette={'High Risk (0-300)': 'red',\n",
        "                 'Medium Risk (301-700)': 'orange',\n",
        "                 'Low Risk (701-1000)': 'green'},\n",
        "        data=tsne_df,\n",
        "        alpha=0.7,\n",
        "        s=50\n",
        "    )\n",
        "    plt.title('Credit Risk Categories (t-SNE Projection)')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend(title='Risk Category')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{REPORT_PATH}kmeans_clustering.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"t-SNE visualization saved\")\n",
        "    return tsne_df\n",
        "\n",
        "\n",
        "def analyze_results(scores_df, cluster_stats_df, features_df):\n",
        "    \"\"\"Generate analysis of credit score distribution and clusters\"\"\"\n",
        "    print(\"Analyzing results...\")\n",
        "\n",
        "    # Score distribution\n",
        "    bins = list(range(0, 1001, 100))\n",
        "    labels = [f\"{i}-{i+99}\" for i in range(0, 1000, 100)]\n",
        "    score_distribution = pd.cut(\n",
        "        scores_df['credit_score'],\n",
        "        bins=bins,\n",
        "        labels=labels,\n",
        "        right=False\n",
        "    ).value_counts().sort_index()\n",
        "\n",
        "    # Plot distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    score_distribution.plot(kind='bar', color='skyblue')\n",
        "    plt.title('Credit Score Distribution')\n",
        "    plt.xlabel('Score Range')\n",
        "    plt.ylabel('Number of Wallets')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{REPORT_PATH}score_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Cluster distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    cluster_dist = scores_df['quality_rank'].value_counts().sort_index()\n",
        "    cluster_dist.plot(kind='bar', color='lightgreen')\n",
        "    plt.title('Wallet Distribution by Behavior Cluster')\n",
        "    plt.xlabel('Cluster Quality Rank (0 = Best)')\n",
        "    plt.ylabel('Number of Wallets')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{REPORT_PATH}cluster_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Calculate statistics\n",
        "    stats = {\n",
        "        'mean_score': scores_df['credit_score'].mean(),\n",
        "        'median_score': scores_df['credit_score'].median(),\n",
        "        'min_score': scores_df['credit_score'].min(),\n",
        "        'max_score': scores_df['credit_score'].max(),\n",
        "        'top_10_percentile': scores_df['credit_score'].quantile(0.9),\n",
        "        'bottom_10_percentile': scores_df['credit_score'].quantile(0.1),\n",
        "        'default_rate': features_df['has_liquidation'].mean()\n",
        "    }\n",
        "\n",
        "    # Generate behavior analysis\n",
        "    cluster_descriptions = {}\n",
        "    for _, row in cluster_stats_df.iterrows():\n",
        "        rank = row['quality_rank']\n",
        "        description = f\"**Cluster {row['cluster']} (Rank {rank})**\\n\"\n",
        "        description += f\"- Size: {row['size']} wallets ({row['size']/len(scores_df)*100:.1f}%)\\n\"\n",
        "        description += f\"- Avg Credit Score: {row['avg_score']:.0f}\\n\"\n",
        "        description += f\"- Avg TX Frequency: {row['avg_tx_freq']:.1f} tx/day\\n\"\n",
        "        description += f\"- Avg Deposit/Borrow Ratio: {row['avg_deposit_borrow_ratio']:.2f}\\n\"\n",
        "        description += f\"- Avg Repay/Borrow Ratio: {row['avg_repay_borrow_ratio']:.2f}\\n\"\n",
        "        description += f\"- Liquidation Rate: {row['liquidation_rate']*100:.1f}%\\n\"\n",
        "        description += f\"- High Frequency Rate: {row['high_freq_rate']*100:.1f}%\\n\"\n",
        "        description += f\"- Risk Score: {row['risk_score']:.3f}\\n\\n\"\n",
        "\n",
        "        # Behavioral characteristics\n",
        "        if rank == 0:\n",
        "            description += \"**Characteristics:**\\n- Responsible long-term users\\n- High deposit/borrow ratios\\n- Consistent repayments\\n- No liquidations\\n- Low transaction frequency\"\n",
        "        elif rank == 1:\n",
        "            description += \"**Characteristics:**\\n- Moderate users\\n- Balanced activity\\n- Occasional borrowing\\n- Rare liquidations\"\n",
        "        elif rank == N_CLUSTERS - 1:\n",
        "            description += \"**Characteristics:**\\n- High-risk behavior\\n- Frequent liquidations\\n- High transaction frequency\\n- Low repay/borrow ratios\\n- Potential bot activity\"\n",
        "        else:\n",
        "            description += \"**Characteristics:**\\n- Mixed behavior patterns\\n- Moderate risk profile\\n- Variable financial ratios\"\n",
        "\n",
        "        cluster_descriptions[rank] = description\n",
        "\n",
        "    # Model calibration plot\n",
        "    prob_true, prob_pred = calibration_curve(\n",
        "        features_df['has_liquidation'],\n",
        "        features_df['default_probability'],\n",
        "        n_bins=10\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(prob_pred, prob_true, 's-', label='Model')\n",
        "    plt.plot([0, 1], [0, 1], '--', color='gray', label='Perfect calibration')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Actual Probability')\n",
        "    plt.title('Model Calibration')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{REPORT_PATH}calibration_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    return score_distribution, stats, cluster_descriptions\n",
        "\n",
        "def build_transaction_network(df):\n",
        "    \"\"\"Build transaction network graph for network analysis\"\"\"\n",
        "    print(\"Building transaction network...\")\n",
        "    G = nx.MultiGraph()\n",
        "\n",
        "    # Add nodes and edges\n",
        "    for _, row in df.iterrows():\n",
        "        wallet = row['userWallet']\n",
        "        protocol = row.get('protocol', 'unknown')\n",
        "\n",
        "        # Add wallet node\n",
        "        G.add_node(wallet, type='wallet')\n",
        "\n",
        "        # Add protocol node\n",
        "        G.add_node(protocol, type='protocol')\n",
        "\n",
        "        # Add transaction edge\n",
        "        G.add_edge(wallet, protocol,\n",
        "                   amount=row['amountUSD'],\n",
        "                   type=row['actionType'],\n",
        "                   timestamp=row['timestamp'])\n",
        "\n",
        "    return G\n",
        "\n",
        "# def calculate_network_metrics(G, features_df):\n",
        "#     \"\"\"Calculate network metrics for each wallet\"\"\"\n",
        "#     print(\"Calculating network metrics...\")\n",
        "\n",
        "#     # Centrality measures\n",
        "#     degree_centrality = nx.degree_centrality(G)\n",
        "#     betweenness_centrality = nx.betweenness_centrality(G, weight='amount')\n",
        "\n",
        "#     # Create metrics dataframe\n",
        "#     wallets = [n for n in G.nodes if G.nodes[n]['type'] == 'wallet']\n",
        "#     network_metrics = pd.DataFrame(index=wallets)\n",
        "#     network_metrics['degree_centrality'] = network_metrics.index.map(degree_centrality)\n",
        "#     network_metrics['betweenness_centrality'] = network_metrics.index.map(betweenness_centrality)\n",
        "\n",
        "#     # Community detection\n",
        "#     communities = nx.algorithms.community.louvain_communities(G, weight='amount', seed=RANDOM_STATE)\n",
        "#     for i, comm in enumerate(communities):\n",
        "#         for node in comm:\n",
        "#             if G.nodes[node]['type'] == 'wallet':\n",
        "#                 network_metrics.loc[node, 'community'] = i\n",
        "\n",
        "#     # Merge with existing features\n",
        "#     features_df = features_df.merge(network_metrics, left_index=True, right_index=True, how='left')\n",
        "#     features_df.fillna({'degree_centrality': 0, 'betweenness_centrality': 0, 'community': -1}, inplace=True)\n",
        "\n",
        "#     return features_df\n",
        "def calculate_network_metrics(G, features_df):\n",
        "    \"\"\"Calculate network metrics for each wallet with optimizations for large networks\"\"\"\n",
        "    print(\"Calculating network metrics...\")\n",
        "\n",
        "    # First get all wallet nodes\n",
        "    wallets = [n for n in G.nodes if G.nodes[n]['type'] == 'wallet']\n",
        "    network_metrics = pd.DataFrame(index=wallets)\n",
        "\n",
        "    # 1. Degree centrality is fast even for large networks\n",
        "    print(\"Calculating degree centrality...\")\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    network_metrics['degree_centrality'] = network_metrics.index.map(degree_centrality)\n",
        "\n",
        "    # 2. Betweenness centrality - use approximation for large networks\n",
        "    print(\"Calculating approximate betweenness centrality...\")\n",
        "    if len(G.nodes) > 1000:\n",
        "        # Use k=100 random nodes for approximation\n",
        "        betweenness = nx.betweenness_centrality(G, k=100, weight='amount', seed=RANDOM_STATE)\n",
        "    else:\n",
        "        betweenness = nx.betweenness_centrality(G, weight='amount')\n",
        "    network_metrics['betweenness_centrality'] = network_metrics.index.map(betweenness)\n",
        "\n",
        "    # 3. Community detection - use Louvain with resolution tuning\n",
        "    print(\"Detecting communities...\")\n",
        "    communities = nx.algorithms.community.louvain_communities(\n",
        "        G,\n",
        "        weight='amount',\n",
        "        resolution=0.8,  # Adjust for more/less communities\n",
        "        seed=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    for i, comm in enumerate(communities):\n",
        "        for node in comm:\n",
        "            if G.nodes[node]['type'] == 'wallet':\n",
        "                network_metrics.loc[node, 'community'] = i\n",
        "\n",
        "    # Merge with existing features\n",
        "    features_df = features_df.merge(\n",
        "        network_metrics,\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='left'\n",
        "    )\n",
        "    features_df.fillna({\n",
        "        'degree_centrality': 0,\n",
        "        'betweenness_centrality': 0,\n",
        "        'community': -1\n",
        "    }, inplace=True)\n",
        "\n",
        "    return features_df\n",
        "\n",
        "\n",
        "class RiskReportGenerator:\n",
        "    def __init__(self, wallet_data, features_df, cluster_stats_df):\n",
        "        self.wallet = wallet_data.name\n",
        "        self.data = wallet_data\n",
        "        self.features_df = features_df\n",
        "        self.cluster_stats_df = cluster_stats_df\n",
        "        self.report_path = f\"{REPORT_PATH}risk_report_{self.wallet[:8]}.pdf\"\n",
        "        self.temp_files = []\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate comprehensive risk assessment report\"\"\"\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", 'B', 24)\n",
        "\n",
        "        # Header\n",
        "        self._add_header(pdf)\n",
        "\n",
        "        # Credit Score Summary\n",
        "        self._add_score_summary(pdf)\n",
        "\n",
        "        # Risk Factors\n",
        "        self._add_risk_factors(pdf)\n",
        "\n",
        "        # Behavioral Analysis\n",
        "        self._add_behavioral_analysis(pdf)\n",
        "\n",
        "        # SHAP Explanation\n",
        "        self._add_shap_explanation(pdf)\n",
        "\n",
        "        # Stress Test Results\n",
        "        self._add_stress_tests(pdf)\n",
        "\n",
        "        # Cluster Comparison\n",
        "        self._add_cluster_comparison(pdf)\n",
        "\n",
        "        # Save report\n",
        "        pdf.output(self.report_path)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        for file in self.temp_files:\n",
        "            if os.path.exists(file):\n",
        "                os.remove(file)\n",
        "\n",
        "        return self.report_path\n",
        "\n",
        "    def _add_header(self, pdf):\n",
        "        \"\"\"Add report header section\"\"\"\n",
        "        pdf.cell(0, 10, \"DeFi Wallet Risk Assessment Report\", ln=1, align='C')\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "        pdf.cell(0, 10, f\"Wallet: {self.wallet}\", ln=1, align='C')\n",
        "        pdf.cell(0, 5, f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", ln=1, align='C')\n",
        "        pdf.ln(10)\n",
        "\n",
        "        # Add risk summary box\n",
        "        pdf.set_fill_color(230, 230, 230)\n",
        "        pdf.cell(0, 8, \"Risk Summary\", ln=1, fill=True)\n",
        "        pdf.set_font(\"Arial\", 'B', 14)\n",
        "\n",
        "        risk_level = \"Low Risk\" if self.data['credit_score'] > 700 else \\\n",
        "                    \"Medium Risk\" if self.data['credit_score'] > 400 else \"High Risk\"\n",
        "\n",
        "        risk_color = (0, 128, 0) if risk_level == \"Low Risk\" else \\\n",
        "                    (255, 165, 0) if risk_level == \"Medium Risk\" else (220, 0, 0)\n",
        "\n",
        "        pdf.set_text_color(*risk_color)\n",
        "        pdf.cell(0, 10, f\"Risk Level: {risk_level}\", ln=1, align='C')\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        pdf.ln(5)\n",
        "\n",
        "    def _add_score_summary(self, pdf):\n",
        "        \"\"\"Add credit score summary section\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Credit Score Summary\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "\n",
        "        # Score visualization\n",
        "        fig, ax = plt.subplots(figsize=(6, 2))\n",
        "        ax.barh(0, self.data['credit_score'], color='skyblue', height=0.5)\n",
        "        ax.set_xlim(0, 1000)\n",
        "        ax.set_xticks([0, 250, 500, 750, 1000])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title('Credit Score')\n",
        "        ax.text(self.data['credit_score'] + 10, 0,\n",
        "                f\"{self.data['credit_score']}\", va='center', fontsize=12)\n",
        "\n",
        "        # Add score ranges\n",
        "        ax.axvline(300, color='red', linestyle='--', alpha=0.3)\n",
        "        ax.axvline(700, color='green', linestyle='--', alpha=0.3)\n",
        "        ax.text(150, 0.7, \"High Risk\", fontsize=8, color='red')\n",
        "        ax.text(500, 0.7, \"Medium Risk\", fontsize=8, color='orange')\n",
        "        ax.text(850, 0.7, \"Low Risk\", fontsize=8, color='green')\n",
        "\n",
        "        # Save to PDF\n",
        "        self._add_figure_to_pdf(pdf, fig)\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Score details\n",
        "        pdf.ln(5)\n",
        "        pdf.cell(0, 8, f\"Score: {self.data['credit_score']}/1000\", ln=1)\n",
        "        pdf.cell(0, 8, f\"Default Probability: {self.data['default_probability']*100:.1f}%\", ln=1)\n",
        "        pdf.cell(0, 8, f\"Percentile: {self._calculate_percentile():.1f}%\", ln=1)\n",
        "        pdf.cell(0, 8, f\"Behavior Cluster: #{self.data['quality_rank']} of {len(self.cluster_stats_df)}\", ln=1)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _add_risk_factors(self, pdf):\n",
        "        \"\"\"Add key risk factors section\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Key Risk Factors\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "\n",
        "        risk_factors = []\n",
        "\n",
        "        # Liquidation risk\n",
        "        if self.data['liquidation_count'] > 0:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: History of liquidations ({self.data['liquidation_count']} events)\"\n",
        "            )\n",
        "\n",
        "        # High frequency\n",
        "        if self.data['high_frequency']:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: High transaction frequency ({self.data['tx_frequency']:.1f} tx/day)\"\n",
        "            )\n",
        "\n",
        "        # Borrowing risk\n",
        "        if self.data['deposit_borrow_ratio'] < 0.5:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: High borrowing relative to deposits (ratio: {self.data['deposit_borrow_ratio']:.2f})\"\n",
        "            )\n",
        "\n",
        "        # Repayment risk\n",
        "        if self.data['repay_borrow_ratio'] < 0.7:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: Low repayment ratio (only {self.data['repay_borrow_ratio']*100:.1f}% of borrows repaid)\"\n",
        "            )\n",
        "\n",
        "        # Utilization risk\n",
        "        if self.data['net_utilization'] < 0.3:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: Low capital utilization ({self.data['net_utilization']*100:.1f}%)\"\n",
        "            )\n",
        "\n",
        "        # Protocol concentration\n",
        "        if self.data['unique_protocols'] < 2:\n",
        "            risk_factors.append(\n",
        "                f\"WARNING: Limited protocol diversity ({self.data['unique_protocols']} protocols)\"\n",
        "            )\n",
        "\n",
        "        # Add risk factors to report\n",
        "        if risk_factors:\n",
        "            for factor in risk_factors:\n",
        "                pdf.cell(0, 8, factor, ln=1)\n",
        "        else:\n",
        "            pdf.cell(0, 8, \"No significant risk factors identified\", ln=1)\n",
        "\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Risk factor visualization\n",
        "        fig, ax = plt.subplots(figsize=(6, 3))\n",
        "        metrics = ['deposit_borrow_ratio', 'repay_borrow_ratio', 'net_utilization', 'protocol_diversity']\n",
        "        values = [self.data[m] for m in metrics]\n",
        "        labels = ['Deposit/Borrow', 'Repay/Borrow', 'Net Utilization', 'Protocol Diversity']\n",
        "\n",
        "        # Calculate reference values\n",
        "        ref_values = [1.5, 1.0, 0.7, 0.5]  # Ideal targets\n",
        "\n",
        "        # Plot comparison\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "        ax.bar(x - width/2, values, width, label='Wallet')\n",
        "        ax.bar(x + width/2, ref_values, width, label='Target', alpha=0.5)\n",
        "\n",
        "        ax.set_ylabel('Ratio')\n",
        "        ax.set_title('Key Financial Ratios')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45)\n",
        "        ax.legend()\n",
        "\n",
        "        self._add_figure_to_pdf(pdf, fig)\n",
        "        plt.close(fig)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _add_behavioral_analysis(self, pdf):\n",
        "        \"\"\"Add behavioral analysis section\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Behavioral Analysis\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "\n",
        "        cluster_id = self.data['cluster']\n",
        "        cluster_data = self.cluster_stats_df[self.cluster_stats_df['cluster'] == cluster_id].iloc[0]\n",
        "\n",
        "        # Cluster description\n",
        "        pdf.cell(0, 8, f\"Behavior Cluster #{self.data['quality_rank']} (of {len(self.cluster_stats_df)} clusters)\", ln=1)\n",
        "        pdf.cell(0, 8, f\"- Cluster Risk Score: {cluster_data['risk_score']:.2f}/1.0\", ln=1)\n",
        "        pdf.cell(0, 8, f\"- Liquidation Rate: {cluster_data['liquidation_rate']*100:.1f}%\", ln=1)\n",
        "        pdf.cell(0, 8, f\"- High Frequency Rate: {cluster_data['high_freq_rate']*100:.1f}%\", ln=1)\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Cluster characteristics\n",
        "        pdf.cell(0, 8, \"Typical Behavior Patterns:\", ln=1)\n",
        "        if self.data['quality_rank'] == 0:\n",
        "            pdf.cell(0, 8, \"- Responsible, long-term user\", ln=1)\n",
        "            pdf.cell(0, 8, \"- Consistent deposit/borrow patterns\", ln=1)\n",
        "            pdf.cell(0, 8, \"- High repayment ratios\", ln=1)\n",
        "        elif self.data['quality_rank'] == len(self.cluster_stats_df) - 1:\n",
        "            pdf.cell(0, 8, \"- High-risk behavior patterns\", ln=1)\n",
        "            pdf.cell(0, 8, \"- Frequent liquidations\", ln=1)\n",
        "            pdf.cell(0, 8, \"- Bot-like transaction patterns\", ln=1)\n",
        "        else:\n",
        "            pdf.cell(0, 8, \"- Moderate risk profile\", ln=1)\n",
        "            pdf.cell(0, 8, \"- Mixed behavior patterns\", ln=1)\n",
        "            pdf.cell(0, 8, \"- Variable financial ratios\", ln=1)\n",
        "\n",
        "        # Cluster comparison visualization\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        clusters = self.cluster_stats_df.sort_values('quality_rank')['quality_rank']\n",
        "        risk_scores = self.cluster_stats_df.sort_values('quality_rank')['risk_score']\n",
        "\n",
        "        colors = []\n",
        "        for rank in clusters:\n",
        "            if rank == 0:\n",
        "                colors.append('green')\n",
        "            elif rank == len(clusters) - 1:\n",
        "                colors.append('red')\n",
        "            else:\n",
        "                colors.append('orange')\n",
        "\n",
        "        ax.bar(clusters.astype(str), risk_scores, color=colors)\n",
        "        ax.set_title('Cluster Risk Comparison')\n",
        "        ax.set_xlabel('Cluster Quality Rank')\n",
        "        ax.set_ylabel('Risk Score')\n",
        "        ax.set_ylim(0, 1)\n",
        "\n",
        "        # Highlight current cluster\n",
        "        current_idx = list(clusters).index(self.data['quality_rank'])\n",
        "        ax.patches[current_idx].set_alpha(0.7)\n",
        "        ax.patches[current_idx].set_edgecolor('black')\n",
        "        ax.patches[current_idx].set_linewidth(2)\n",
        "\n",
        "        self._add_figure_to_pdf(pdf, fig)\n",
        "        plt.close(fig)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _add_shap_explanation(self, pdf):\n",
        "        \"\"\"Add SHAP explanation for credit score\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Credit Score Explanation\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "        pdf.cell(0, 8, \"Factors influencing this wallet's credit score:\", ln=1)\n",
        "\n",
        "        # Get SHAP values\n",
        "        if 'shap_values' in self.data:\n",
        "            shap_values = self.data['shap_values']\n",
        "            feature_names = [\n",
        "                'Total TX', 'Deposit Count', 'Borrow Count', 'Repay Count',\n",
        "                'TX Frequency', 'Deposit/Borrow', 'Repay/Borrow',\n",
        "                'Redeem/Deposit', 'Net Utilization', 'Has Liquidation',\n",
        "                'High Frequency', 'Recency Days', 'TX Trend', 'Protocol Diversity',\n",
        "                'Borrow/Recency', 'Deposit/Frequency'\n",
        "            ]\n",
        "\n",
        "            # Create waterfall plot\n",
        "            plt.figure()\n",
        "            shap.plots.waterfall(shap_values, max_display=10, show=False)\n",
        "            plt.title(\"Credit Score Factors\", fontsize=14)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save to PDF\n",
        "            temp_path = f\"{REPORT_PATH}temp_shap_{self.wallet[:6]}.png\"\n",
        "            plt.savefig(temp_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            self.temp_files.append(temp_path)\n",
        "\n",
        "            pdf.image(temp_path, x=10, w=190)\n",
        "            pdf.ln(5)\n",
        "        else:\n",
        "            pdf.cell(0, 8, \"SHAP explanation not available\", ln=1)\n",
        "\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _add_stress_tests(self, pdf):\n",
        "        \"\"\"Add stress test results section with real market data\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Stress Test Results\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "\n",
        "        # Get current market data\n",
        "        try:\n",
        "            eth = yf.Ticker(\"ETH-USD\")\n",
        "            eth_history = eth.history(period=\"1d\")\n",
        "            eth_price = eth_history['Close'].iloc[0]\n",
        "            btc = yf.Ticker(\"BTC-USD\")\n",
        "            btc_history = btc.history(period=\"1d\")\n",
        "            btc_price = btc_history['Close'].iloc[0]\n",
        "            vix = yf.Ticker(\"^VIX\")\n",
        "            vix_history = vix.history(period=\"1d\")\n",
        "            vix_value = vix_history['Close'].iloc[0]\n",
        "        except:\n",
        "            eth_price = 3500\n",
        "            btc_price = 60000\n",
        "            vix_value = 20\n",
        "\n",
        "        # Simulate different market conditions\n",
        "        scenarios = [\n",
        "            {\"name\": \"Normal Market\",\n",
        "             \"volatility\": 0.0,\n",
        "             \"collateral_factor\": 1.0,\n",
        "             \"eth_price\": eth_price,\n",
        "             \"btc_price\": btc_price,\n",
        "             \"vix\": vix_value},\n",
        "\n",
        "            {\"name\": \"Moderate Volatility\",\n",
        "             \"volatility\": 0.2 * (vix_value/20),\n",
        "             \"collateral_factor\": 0.85,\n",
        "             \"eth_price\": eth_price * 0.9,\n",
        "             \"btc_price\": btc_price * 0.9,\n",
        "             \"vix\": vix_value * 1.5},\n",
        "\n",
        "            {\"name\": \"Market Crash\",\n",
        "             \"volatility\": 0.5 * (vix_value/20),\n",
        "             \"collateral_factor\": 0.6,\n",
        "             \"eth_price\": eth_price * 0.7,\n",
        "             \"btc_price\": btc_price * 0.65,\n",
        "             \"vix\": vix_value * 2.5},\n",
        "\n",
        "            {\"name\": \"Extreme Conditions\",\n",
        "             \"volatility\": 0.8 * (vix_value/20),\n",
        "             \"collateral_factor\": 0.4,\n",
        "             \"eth_price\": eth_price * 0.5,\n",
        "             \"btc_price\": btc_price * 0.45,\n",
        "             \"vix\": vix_value * 3.5}\n",
        "        ]\n",
        "\n",
        "        results = []\n",
        "        for scenario in scenarios:\n",
        "            health, prob = self._simulate_stress_test(scenario)\n",
        "            results.append({\n",
        "                \"Scenario\": scenario['name'],\n",
        "                \"ETH Price\": f\"${scenario['eth_price']:,.0f}\",\n",
        "                \"VIX Index\": f\"{scenario['vix']:.1f}\",\n",
        "                \"Health Factor\": health,\n",
        "                \"Liquidation Prob\": prob\n",
        "            })\n",
        "\n",
        "        # Create table\n",
        "        pdf.set_fill_color(240, 240, 240)\n",
        "        pdf.cell(50, 8, \"Scenario\", border=1, fill=True)\n",
        "        pdf.cell(30, 8, \"ETH Price\", border=1, fill=True)\n",
        "        pdf.cell(25, 8, \"VIX\", border=1, fill=True)\n",
        "        pdf.cell(35, 8, \"Health Factor\", border=1, fill=True)\n",
        "        pdf.cell(35, 8, \"Liquidation Prob\", border=1, fill=True, ln=1)\n",
        "\n",
        "        for res in results:\n",
        "            health_color = (0, 100, 0) if res['Health Factor'] > 1.5 else \\\n",
        "                          (205, 133, 0) if res['Health Factor'] > 1.0 else (220, 0, 0)\n",
        "\n",
        "            prob_color = (220, 0, 0) if res['Liquidation Prob'] > 0.7 else \\\n",
        "                        (205, 133, 0) if res['Liquidation Prob'] > 0.3 else (0, 100, 0)\n",
        "\n",
        "            pdf.set_text_color(0, 0, 0)\n",
        "            pdf.cell(50, 8, res['Scenario'], border=1)\n",
        "\n",
        "            pdf.cell(30, 8, res['ETH Price'], border=1)\n",
        "            pdf.cell(25, 8, res['VIX Index'], border=1)\n",
        "\n",
        "            pdf.set_text_color(*health_color)\n",
        "            pdf.cell(35, 8, f\"{res['Health Factor']:.2f}\", border=1)\n",
        "\n",
        "            pdf.set_text_color(*prob_color)\n",
        "            pdf.cell(35, 8, f\"{res['Liquidation Prob']:.0%}\", border=1, ln=1)\n",
        "\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        pdf.ln(5)\n",
        "\n",
        "        # Add scenario description\n",
        "        pdf.cell(0, 8, \"Scenario Definitions:\", ln=1)\n",
        "        pdf.cell(0, 8, \"- Normal Market: Stable market conditions\", ln=1)\n",
        "        pdf.cell(0, 8, \"- Moderate Volatility: 10-20% price decline, VIX 30-40\", ln=1)\n",
        "        pdf.cell(0, 8, \"- Market Crash: 30-50% price decline, VIX 50+\", ln=1)\n",
        "        pdf.cell(0, 8, \"- Extreme Conditions: >50% price decline, VIX 70+\", ln=1)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _add_cluster_comparison(self, pdf):\n",
        "        \"\"\"Add cluster comparison section\"\"\"\n",
        "        pdf.set_font(\"Arial\", 'B', 16)\n",
        "        pdf.cell(0, 10, \"Peer Group Comparison\", ln=1)\n",
        "        pdf.set_font(\"Arial\", '', 12)\n",
        "\n",
        "        cluster_id = self.data['cluster']\n",
        "        cluster_data = self.cluster_stats_df[self.cluster_stats_df['cluster'] == cluster_id].iloc[0]\n",
        "\n",
        "        # Create comparison table\n",
        "        metrics = [\n",
        "            ('Credit Score', self.data['credit_score'], cluster_data.get('avg_score', 0)),\n",
        "            ('Default Probability', self.data['default_probability'], cluster_data.get('default_probability', 0)),\n",
        "            ('TX Frequency', self.data['tx_frequency'], cluster_data.get('avg_tx_freq', 0)),\n",
        "            ('Deposit/Borrow Ratio', self.data['deposit_borrow_ratio'], cluster_data.get('avg_deposit_borrow_ratio', 0)),\n",
        "            ('Liquidation Events', self.data['liquidation_count'], cluster_data.get('avg_liquidation', 0)),\n",
        "            ('Protocol Diversity', self.data['protocol_diversity'], cluster_data.get('avg_protocol_diversity', 0))\n",
        "        ]\n",
        "\n",
        "        # Create table\n",
        "        pdf.set_fill_color(240, 240, 240)\n",
        "        pdf.cell(70, 8, \"Metric\", border=1, fill=True)\n",
        "        pdf.cell(40, 8, \"Wallet\", border=1, fill=True)\n",
        "        pdf.cell(40, 8, \"Cluster Avg\", border=1, fill=True, ln=1)\n",
        "\n",
        "        for metric, wallet_val, cluster_avg in metrics:\n",
        "            # Determine value colors\n",
        "            if metric == 'Credit Score':\n",
        "                wallet_color = (0, 100, 0) if wallet_val > 700 else \\\n",
        "                              (205, 133, 0) if wallet_val > 400 else (220, 0, 0)\n",
        "                cluster_color = (0, 100, 0) if cluster_avg > 700 else \\\n",
        "                               (205, 133, 0) if cluster_avg > 400 else (220, 0, 0)\n",
        "            elif metric == 'Default Probability':\n",
        "                wallet_color = (220, 0, 0) if wallet_val > 0.7 else \\\n",
        "                              (205, 133, 0) if wallet_val > 0.3 else (0, 100, 0)\n",
        "                cluster_color = (220, 0, 0) if cluster_avg > 0.7 else \\\n",
        "                               (205, 133, 0) if cluster_avg > 0.3 else (0, 100, 0)\n",
        "            else:\n",
        "                wallet_color = (0, 0, 0)\n",
        "                cluster_color = (0, 0, 0)\n",
        "\n",
        "            pdf.set_text_color(0, 0, 0)\n",
        "            pdf.cell(70, 8, metric, border=1)\n",
        "\n",
        "            pdf.set_text_color(*wallet_color)\n",
        "            pdf.cell(40, 8, f\"{wallet_val:.2f}\", border=1)\n",
        "\n",
        "            pdf.set_text_color(*cluster_color)\n",
        "            pdf.cell(40, 8, f\"{cluster_avg:.2f}\", border=1, ln=1)\n",
        "\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        pdf.ln(10)\n",
        "\n",
        "    def _simulate_stress_test(self, scenario):\n",
        "        \"\"\"Simulate wallet performance under market stress\"\"\"\n",
        "        # Base health factor estimate\n",
        "        health_factor = max(1.0, 1.5 * self.data['deposit_borrow_ratio'])\n",
        "\n",
        "        # Apply volatility impact\n",
        "        volatility_impact = scenario['volatility'] * (1 - self.data['protocol_diversity'])\n",
        "        stressed_health = health_factor * (1 - volatility_impact)\n",
        "\n",
        "        # Apply collateral haircut\n",
        "        stressed_health *= scenario['collateral_factor']\n",
        "\n",
        "        # Calculate liquidation probability\n",
        "        if stressed_health < 1.0:\n",
        "            liquidation_prob = min(1.0, (1.0 - stressed_health) * 2)\n",
        "        else:\n",
        "            liquidation_prob = max(0, (1.5 - stressed_health) / 5)\n",
        "\n",
        "        return stressed_health, liquidation_prob\n",
        "\n",
        "    def _calculate_percentile(self):\n",
        "        \"\"\"Calculate wallet's percentile rank\"\"\"\n",
        "        scores = self.features_df['credit_score']\n",
        "        return (sum(scores < self.data['credit_score']) / len(scores)) * 100\n",
        "\n",
        "    def _add_figure_to_pdf(self, pdf, figure):\n",
        "        \"\"\"Add matplotlib figure to PDF\"\"\"\n",
        "        # Save as temp image\n",
        "        temp_path = f\"{REPORT_PATH}temp_{self.wallet[:6]}.png\"\n",
        "        figure.savefig(temp_path, dpi=100, bbox_inches='tight')\n",
        "        self.temp_files.append(temp_path)\n",
        "\n",
        "        # Add to PDF\n",
        "        pdf.image(temp_path, x=10, w=190)\n",
        "        pdf.ln(5)\n",
        "\n",
        "def backtest_model(features_df):\n",
        "    \"\"\"Backtest model performance using temporal splits\"\"\"\n",
        "    print(\"Backtesting model performance...\")\n",
        "\n",
        "    # Sort by time\n",
        "    features_df = features_df.sort_values('last_tx')\n",
        "\n",
        "    # Create results storage\n",
        "    backtest_results = []\n",
        "\n",
        "    # Get numeric features only (exclude timestamps and other non-numeric columns)\n",
        "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'has_liquidation' not in numeric_cols:\n",
        "        numeric_cols.append('has_liquidation')\n",
        "\n",
        "    # Temporal cross-validation\n",
        "    for test_size in [0.1, 0.2, 0.3]:\n",
        "        train_size = 1 - test_size\n",
        "        split_idx = int(train_size * len(features_df))\n",
        "\n",
        "        X_train = features_df[numeric_cols].iloc[:split_idx]\n",
        "        X_test = features_df[numeric_cols].iloc[split_idx:]\n",
        "\n",
        "        # Train new model\n",
        "        gbm = GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=3,\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        # Fit and predict\n",
        "        gbm.fit(X_train.drop('has_liquidation', axis=1), X_train['has_liquidation'])\n",
        "        y_pred = gbm.predict_proba(X_test.drop('has_liquidation', axis=1))[:, 1]\n",
        "        auc = roc_auc_score(X_test['has_liquidation'], y_pred)\n",
        "\n",
        "        backtest_results.append({\n",
        "            'train_size': train_size,\n",
        "            'test_size': test_size,\n",
        "            'auc': auc\n",
        "        })\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sizes = [res['test_size'] for res in backtest_results]\n",
        "    aucs = [res['auc'] for res in backtest_results]\n",
        "    plt.plot(sizes, aucs, 'o-')\n",
        "    plt.title('Model Performance by Test Size')\n",
        "    plt.xlabel('Test Set Proportion')\n",
        "    plt.ylabel('AUC Score')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{REPORT_PATH}backtest_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "    return pd.DataFrame(backtest_results)\n",
        "\n",
        "def detect_anomalies(features_df):\n",
        "    \"\"\"Detect anomalous wallets using isolation forest\"\"\"\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "\n",
        "    print(\"Detecting anomalous wallets...\")\n",
        "\n",
        "    # Select features for anomaly detection\n",
        "    anomaly_features = [\n",
        "        'tx_frequency', 'deposit_borrow_ratio', 'repay_borrow_ratio',\n",
        "        'net_utilization', 'tx_trend', 'protocol_diversity'\n",
        "    ]\n",
        "    X_anom = features_df[anomaly_features].fillna(0)\n",
        "\n",
        "    # Train isolation forest\n",
        "    iso = IsolationForest(contamination=0.01, random_state=RANDOM_STATE)\n",
        "    anomalies = iso.fit_predict(X_anom)\n",
        "\n",
        "    # Add to features\n",
        "    features_df['is_anomaly'] = (anomalies == -1).astype(int)\n",
        "\n",
        "    # Plot anomalies\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(\n",
        "        features_df['tx_frequency'],\n",
        "        features_df['deposit_borrow_ratio'],\n",
        "        c=features_df['is_anomaly'],\n",
        "        cmap='coolwarm',\n",
        "        alpha=0.6\n",
        "    )\n",
        "    plt.xlabel('Transaction Frequency')\n",
        "    plt.ylabel('Deposit/Borrow Ratio')\n",
        "    plt.title('Anomaly Detection')\n",
        "    plt.colorbar(label='Anomaly (1=True)')\n",
        "    plt.savefig(f'{REPORT_PATH}anomaly_detection.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Detected {features_df['is_anomaly'].sum()} anomalous wallets\")\n",
        "    return features_df\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_json('user-wallet-transactions.json')\n",
        "        print(f\"Loaded {len(df):,} transactions\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Process transactions and extract features\n",
        "    features_df = process_transactions(df)\n",
        "\n",
        "    # Build transaction network\n",
        "    G = build_transaction_network(df)\n",
        "    features_df = calculate_network_metrics(G, features_df)\n",
        "\n",
        "    # Detect anomalies\n",
        "    features_df = detect_anomalies(features_df)\n",
        "\n",
        "    # Generate credit scores\n",
        "    scores_df, cluster_stats_df, X_scaled, full_features_df = generate_credit_scores(features_df)\n",
        "\n",
        "    # Save results\n",
        "    scores_df.to_csv(f'{REPORT_PATH}wallet_credit_scores.csv', index=False)\n",
        "    cluster_stats_df.to_csv(f'{REPORT_PATH}cluster_statistics.csv', index=False)\n",
        "    full_features_df.to_csv(f'{REPORT_PATH}full_features.csv', index=True)\n",
        "    print(f\"Saved scores for {len(scores_df)} wallets\")\n",
        "\n",
        "    # Generate t-SNE visualization\n",
        "    try:\n",
        "        tsne_df = generate_tsne_visualization(X_scaled, scores_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate t-SNE: {e}\")\n",
        "\n",
        "    # Analyze results\n",
        "    distribution, stats, cluster_descriptions = analyze_results(scores_df, cluster_stats_df, full_features_df)\n",
        "\n",
        "    # Backtest model\n",
        "    backtest_results = backtest_model(full_features_df)\n",
        "    backtest_results.to_csv(f'{REPORT_PATH}backtest_results.csv', index=False)\n",
        "\n",
        "    # Generate analysis report\n",
        "    with open(f'{REPORT_PATH}analysis.md', 'w') as f:\n",
        "        f.write(\"# DeFi Credit Score Analysis Report\\n\\n\")\n",
        "\n",
        "        f.write(\"## Summary Statistics\\n\")\n",
        "        f.write(pd.Series(stats).to_markdown() + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"## Score Distribution\\n\")\n",
        "        f.write(distribution.to_markdown() + \"\\n\")\n",
        "        f.write(\"![Score Distribution](score_distribution.png)\\n\\n\")\n",
        "\n",
        "        f.write(\"## Model Performance\\n\")\n",
        "        f.write(backtest_results.to_markdown() + \"\\n\")\n",
        "        f.write(\"![Backtest Performance](backtest_performance.png)\\n\\n\")\n",
        "\n",
        "        f.write(\"## Cluster Characteristics\\n\")\n",
        "        for rank in sorted(cluster_descriptions.keys()):\n",
        "            f.write(cluster_descriptions[rank] + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"## Risk Analysis\\n\")\n",
        "        f.write(\"### High-Score Wallets (700-1000)\\n\")\n",
        "        f.write(\"- Typically in top behavior clusters (Rank 0-1)\\n\")\n",
        "        f.write(\"- Consistent deposit/repay patterns\\n\")\n",
        "        f.write(\"- High repay/borrow ratios (>1.5)\\n\")\n",
        "        f.write(\"- No liquidation history\\n\")\n",
        "        f.write(\"- Moderate transaction frequency\\n\\n\")\n",
        "\n",
        "        f.write(\"### Medium-Score Wallets (300-699)\\n\")\n",
        "        f.write(\"- Mixed behavior patterns\\n\")\n",
        "        f.write(\"- Moderate deposit/borrow ratios (0.5-1.5)\\n\")\n",
        "        f.write(\"- Occasional borrowing without full repayment\\n\")\n",
        "        f.write(\"- Rare liquidation events\\n\\n\")\n",
        "\n",
        "        f.write(\"### Low-Score Wallets (0-299)\\n\")\n",
        "        f.write(\"- High-risk behavior patterns\\n\")\n",
        "        f.write(\"- Frequent liquidations\\n\")\n",
        "        f.write(\"- Extremely high or low transaction frequency\\n\")\n",
        "        f.write(\"- Low repay/borrow ratios (<0.5)\\n\")\n",
        "        f.write(\"- Often show bot-like behavior patterns\\n\")\n",
        "\n",
        "    # Generate sample risk reports\n",
        "    print(\"Generating sample risk assessment reports...\")\n",
        "    reports = []\n",
        "    for wallet in list(scores_df.head(3)['wallet']) + list(scores_df.tail(3)['wallet']):\n",
        "        wallet_data = full_features_df.loc[wallet]\n",
        "        report_gen = RiskReportGenerator(wallet_data, full_features_df, cluster_stats_df)\n",
        "        report_path = report_gen.generate_report()\n",
        "        reports.append(report_path)\n",
        "        print(f\"Generated report for {wallet[:8]}...\")\n",
        "\n",
        "    print(\"\\nProcessing complete! Results saved to:\")\n",
        "    print(f\"- {REPORT_PATH}wallet_credit_scores.csv\")\n",
        "    print(f\"- {REPORT_PATH}cluster_statistics.csv\")\n",
        "    print(f\"- {REPORT_PATH}analysis.md\")\n",
        "    print(f\"- {REPORT_PATH}backtest_results.csv\")\n",
        "    print(f\"- Various visualizations and reports\")\n",
        "    print(f\"- {len(reports)} sample risk assessment reports\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnEj6175zeC9",
        "outputId": "83c961ae-56b5-4e63-bf43-ca5416bc5d83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 100,000 transactions\n",
            "Processing transactions...\n",
            "Performing vectorized feature engineering...\n",
            "Processed features for 3497 wallets\n",
            "Building transaction network...\n",
            "Calculating network metrics...\n",
            "Calculating degree centrality...\n",
            "Calculating approximate betweenness centrality...\n",
            "Detecting communities...\n",
            "Detecting anomalous wallets...\n",
            "Detected 34 anomalous wallets\n",
            "Generating credit scores...\n",
            "Optimizing cluster count...\n",
            "Silhouette score for 2 clusters: 0.9637\n",
            "Silhouette score for 3 clusters: 0.7128\n",
            "Silhouette score for 4 clusters: 0.5917\n",
            "Silhouette score for 5 clusters: 0.4699\n",
            "Silhouette score for 6 clusters: 0.3567\n",
            "Silhouette score for 7 clusters: 0.3592\n",
            "Silhouette score for 8 clusters: 0.4397\n",
            "Silhouette score for 9 clusters: 0.4767\n",
            "Optimal clusters: 2\n",
            "Clustering with 2 clusters using differential privacy...\n",
            "Training supervised credit model...\n",
            "Model AUC: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       683\n",
            "           1       1.00      1.00      1.00        17\n",
            "\n",
            "    accuracy                           1.00       700\n",
            "   macro avg       1.00      1.00      1.00       700\n",
            "weighted avg       1.00      1.00      1.00       700\n",
            "\n",
            "Calculating SHAP values...\n",
            "Saved scores for 3497 wallets\n",
            "Generating t-SNE visualization...\n",
            "t-SNE visualization saved\n",
            "Analyzing results...\n",
            "Backtesting model performance...\n",
            "Generating sample risk assessment reports...\n",
            "Generated report for 0x0298b2...\n",
            "Generated report for 0x00fea8...\n",
            "Generated report for 0x05d19d...\n",
            "Generated report for 0x04dc49...\n",
            "Generated report for 0x00296e...\n",
            "Generated report for 0x04963f...\n",
            "\n",
            "Processing complete! Results saved to:\n",
            "- reports/wallet_credit_scores.csv\n",
            "- reports/cluster_statistics.csv\n",
            "- reports/analysis.md\n",
            "- reports/backtest_results.csv\n",
            "- Various visualizations and reports\n",
            "- 6 sample risk assessment reports\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}